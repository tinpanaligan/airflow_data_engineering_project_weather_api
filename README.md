# Weather Data ETL Pipeline with Airflow

---

## Overview

This project involves building an ETL (Extract, Transform, Load) pipeline for weather data using Python, Pandas, Apache Airflow, AWS EC2, and AWS S3. The pipeline connects to a Weather API to extract real-time weather data, processes it using Pandas in an ETL Python file, schedules the ETL process with a DAG (Directed Acyclic Graph) Python file for Apache Airflow, and finally saves the resulting data as CSV files into AWS S3. The data streaming process is scheduled to repeat daily for continuous updates.

![Airflow_Project_Architecture](https://github.com/tinpanaligan/airflow_data_engineering_project_weather_api/assets/116711183/fa75d9c3-e8ab-4305-9bec-fbb1776337c7)

---

## Components

### 1. Weather API

- **Source**: Real-time weather data retrieved from [Weather API](https://www.weatherapi.com/).
- **Purpose**: To extract current weather information for processing.

### 2. ETL Python File

- **Purpose**: Python script utilizing Pandas to perform Extract, Transform, and Load operations on the weather data.
- **Execution**: Converts raw data into a structured format suitable for analysis and storage.

### 3. DAG Python File for Airflow

- **Purpose**: Defines the workflow as a Directed Acyclic Graph for Apache Airflow.
- **Execution**: Schedules and orchestrates the ETL process, ensuring it runs daily on a defined schedule.

### 4. Apache Airflow

- **Purpose**: Airflow is used as a platform to programmatically author, schedule, and monitor workflows.
- **Deployment**: Airflow is run on an AWS EC2 instance for scheduling and executing the ETL pipeline.

### 5. AWS EC2 Instance

- **Purpose**: To host Apache Airflow for workflow orchestration and scheduling.
- **Deployment**: Utilize AWS EC2 for its scalability and reliability.

### 6. AWS S3

- **Purpose**: AWS S3 is used for storing the processed weather data in CSV format.
- **Storage**: CSV files generated by the ETL process are saved into AWS S3 buckets for accessibility and long-term storage.

---

## How to Run

1. **Connect to Weather API**:
   - Obtain access to a Weather API and obtain necessary API keys or credentials.

2. **Set Up Apache Airflow on AWS EC2**:
   - Launch an EC2 instance on AWS.
   - Install and configure Apache Airflow on the EC2 instance.

3. **Configure Airflow DAG**:
   - Define the DAG (Directed Acyclic Graph) using a Python file to schedule and orchestrate the ETL process.
   - Ensure the DAG triggers the ETL Python file at the desired schedule (e.g., daily).

4. **Create ETL Python Script**:
   - Develop a Python script using Pandas to extract, transform, and load weather data from the API.
   - Ensure the script produces CSV files as output.

5. **Schedule ETL Process**:
   - Configure Airflow to execute the ETL Python script according to the defined DAG schedule.

6. **Save Data to AWS S3**:
   - Ensure the ETL Python script saves the processed data as CSV files into AWS S3 buckets.

---

## Project Documentation

[Project Documentation - Apache Airflow Weather Data API Data Pipeline](https://alpine-dollar-b3b.notion.site/Project-Documentation-Apache-Airflow-Weather-Data-API-Data-Pipeline-b4892e09fb8d4c76a31b4e7adc7de98b?pvs=4)

## Contributors

- Tin Panaligan

---

## Acknowledgments

Special thanks to Darshil Parmar for this project.

---

## Contact Information

For inquiries, please contact tinpanaligan.work@gmail.com.

---

## Appendix: Useful Resources

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [AWS EC2 Documentation](https://docs.aws.amazon.com/ec2/index.html)
- [AWS S3 Documentation](https://docs.aws.amazon.com/s3/index.html)

---
